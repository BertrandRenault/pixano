{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"/home/maximilien/work/pixano/\")\n",
        "sys.path.append(\"/home/maximilien/work/lib/bop_toolkit\")\n",
        "\n",
        "dir = \"/home/maximilien/work/bop_data/\"\n",
        "coco = \"/home/maximilien/work/adapt-2023_with_gt/test/coco_gt.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import bop_toolkit_lib.dataset.bop_webdataset as btk\n",
        "import lance\n",
        "import pyarrow as pa\n",
        "import webdataset as wds\n",
        "from PIL import Image as pilImage\n",
        "\n",
        "from pixano.core import (\n",
        "    BBox,\n",
        "    Camera,\n",
        "    CameraType,\n",
        "    CompressedRLE,\n",
        "    CompressedRLEType,\n",
        "    DepthImage,\n",
        "    DepthImageType,\n",
        "    GtInfo,\n",
        "    GtInfoType,\n",
        "    Image,\n",
        "    ImageType,\n",
        "    Pose,\n",
        "    PoseType,\n",
        ")\n",
        "from pixano.data import DatasetInfo, Fields, Importer\n",
        "from pixano.utils import image_to_binary, image_to_thumbnail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BOPImporter(Importer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        shard_split: dict[str, list[str]],\n",
        "        info: DatasetInfo,\n",
        "        target_dir: Path,\n",
        "    ):\n",
        "        self.shard_split = shard_split\n",
        "        self.info = info\n",
        "        self.target_dir = target_dir\n",
        "\n",
        "    @property\n",
        "    def fields(self) -> Fields:\n",
        "        return Fields.from_dict(self.info.fields)\n",
        "\n",
        "    def create_json(self):\n",
        "        \"\"\"Create dataset spec.json\"\"\"\n",
        "\n",
        "        # Check number of rows in the created dataset\n",
        "        # self.info.num_elements = dataset.count_rows()\n",
        "\n",
        "        # Create spec.json\n",
        "        with open(self.target_dir + \"/spec.json\", \"w\") as f:\n",
        "            json.dump(vars(self.info), f, indent=4)\n",
        "\n",
        "    def schema(self):\n",
        "        return pa.schema(self.fields.to_pyarrow())\n",
        "\n",
        "    def import_row(self):\n",
        "        # split dataset\n",
        "        for split, shard_list in self.shard_split.items():\n",
        "            print(split)\n",
        "\n",
        "            _wds_pipeline = wds.DataPipeline(\n",
        "                wds.SimpleShardList(shard_list), wds.tarfile_to_samples()\n",
        "            )\n",
        "\n",
        "            # extract row of each split\n",
        "            for n, row in enumerate(_wds_pipeline):\n",
        "                if True:\n",
        "                    sample = btk.decode_sample(\n",
        "                        row,\n",
        "                        decode_camera=True,\n",
        "                        decode_rgb=True,\n",
        "                        decode_gray=False,\n",
        "                        decode_depth=True,\n",
        "                        decode_gt=True,\n",
        "                        decode_gt_info=True,\n",
        "                        decode_mask_visib=False,\n",
        "                        decode_mask=False,\n",
        "                        rgb_suffix=\".png\",\n",
        "                    )\n",
        "\n",
        "                    # id\n",
        "                    id = row[\"__key__\"]\n",
        "\n",
        "                    scene, image = id.split(\"_\")\n",
        "                    coco_json_path = f\"/home/maximilien/work/adapt-2023_with_gt/{split}/{scene}/scene_gt_coco.json\"\n",
        "\n",
        "                    # rgb\n",
        "                    im_pil = pilImage.fromarray(sample[\"im_rgb\"])\n",
        "\n",
        "                    im_pil = image_to_binary(im_pil, format=\"JPEG\")\n",
        "\n",
        "                    preview = image_to_thumbnail(im_pil)\n",
        "\n",
        "                    rgb = Image(f\"\", im_pil, preview)\n",
        "                    rgbs = ImageType.Array.from_list([rgb])\n",
        "                    # dept\n",
        "                    depths = DepthImageType.Array.from_list(\n",
        "                        [\n",
        "                            DepthImage(\n",
        "                                depth_map=sample[\"im_depth\"],\n",
        "                                shape=sample[\"im_depth\"].shape,\n",
        "                            )\n",
        "                        ]\n",
        "                    )\n",
        "                    # camera\n",
        "                    cameras = CameraType.Array.from_list(\n",
        "                        [Camera.from_dict(sample[\"camera\"])]\n",
        "                    )\n",
        "\n",
        "                    # Objects\n",
        "                    nb_object = len(sample[\"gt\"])\n",
        "                    # category\n",
        "                    category_id = [\n",
        "                        sample[\"gt\"][i][\"object_id\"] for i in range(nb_object)\n",
        "                    ]\n",
        "                    category_id_arr = pa.array([category_id])\n",
        "\n",
        "                    # pose\n",
        "                    gt = [\n",
        "                        Pose(\n",
        "                            sample[\"gt\"][i][\"cam_R_m2c\"].flatten(),\n",
        "                            sample[\"gt\"][i][\"cam_t_m2c\"].flatten(),\n",
        "                        )\n",
        "                        for i in range(nb_object)\n",
        "                    ]\n",
        "                    gt_arr = PoseType.Array.from_lists([gt])\n",
        "\n",
        "                    # gt_info\n",
        "                    gt_infos = [\n",
        "                        GtInfo.from_dict(\n",
        "                            {\n",
        "                                **sample[\"gt_info\"][i],\n",
        "                                \"bbox_obj\": BBox.from_xywh(\n",
        "                                    sample[\"gt_info\"][i][\"bbox_obj\"]\n",
        "                                ),\n",
        "                                \"bbox_visib\": BBox.from_xywh(\n",
        "                                    sample[\"gt_info\"][i][\"bbox_visib\"]\n",
        "                                ),\n",
        "                            }\n",
        "                        )\n",
        "                        for i in range(nb_object)\n",
        "                    ]\n",
        "                    gt_infos_arr = GtInfoType.Array.from_lists([gt_infos])\n",
        "\n",
        "                    # objects_ids and masks\n",
        "                    with open(coco_json_path, \"r\") as f:\n",
        "                        data = json.load(f)\n",
        "\n",
        "                    object_ids = []\n",
        "                    masks = []\n",
        "                    for ann in data[\"annotations\"]:\n",
        "                        # check if same image key, then annotations are in same order as other object's attribute in coco.json\n",
        "                        if \"000\" + ann[\"image_id\"] == id.replace(\"_\", \"-\"):\n",
        "                            object_ids.append(ann[\"id\"])\n",
        "                            masks.append(\n",
        "                                CompressedRLE.from_urle(\n",
        "                                    ann[\"segmentation\"],\n",
        "                                    ann[\"segmentation\"][\"size\"][0],\n",
        "                                    ann[\"segmentation\"][\"size\"][1],\n",
        "                                )\n",
        "                            )\n",
        "\n",
        "                    masks_arr = CompressedRLEType.Array.from_lists([masks])\n",
        "                    object_ids_arr = pa.array([object_ids])\n",
        "\n",
        "                    # Struct array\n",
        "                    struct_arr = pa.StructArray.from_arrays(\n",
        "                        [\n",
        "                            pa.array([id]),\n",
        "                            rgbs,\n",
        "                            depths,\n",
        "                            cameras,\n",
        "                            category_id_arr,\n",
        "                            object_ids_arr,\n",
        "                            masks_arr,\n",
        "                            gt_arr,\n",
        "                            gt_infos_arr,\n",
        "                            pa.array([split]),\n",
        "                        ],\n",
        "                        fields=self.fields(),\n",
        "                    )\n",
        "\n",
        "                    yield pa.RecordBatch.from_struct_array(struct_arr)\n",
        "\n",
        "    def import_dataset(\n",
        "        self, max_rows_per_file: int = 1024 * 1024, max_rows_per_group: int = 1024\n",
        "    ) -> lance.LanceDataset:\n",
        "        \"\"\"Import dataset to Pixano format\n",
        "\n",
        "        Args:\n",
        "            batch_size (int, optional): Number of rows per file. Defaults to 2048.\n",
        "        \"\"\"\n",
        "        reader = pa.RecordBatchReader.from_batches(self.schema(), self.import_row())\n",
        "        ds = lance.write_dataset(reader, self.target_dir)\n",
        "\n",
        "        self.info.num_elements = ds.count_rows()\n",
        "\n",
        "        # Create spec.json\n",
        "        self.create_json()\n",
        "\n",
        "        return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "shard_test_dir = \"/home/maximilien/work/adapt-2023_with_gt/shard/shard_test/\"\n",
        "shard_test_list = [\n",
        "    os.path.join(shard_test_dir, shard)\n",
        "    for shard in os.listdir(shard_test_dir)\n",
        "    if shard.endswith(\".tar\")\n",
        "]\n",
        "\n",
        "shard_validation_dir = (\n",
        "    \"/home/maximilien/work/adapt-2023_with_gt/shard/shard_validation/\"\n",
        ")\n",
        "shard_validation_list = [\n",
        "    os.path.join(shard_validation_dir, shard)\n",
        "    for shard in os.listdir(shard_validation_dir)\n",
        "    if shard.endswith(\".tar\")\n",
        "]\n",
        "\n",
        "shard_split = {\"test\": shard_test_list, \"val\": shard_validation_list}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tempfile\n",
        "from pixano.data import BopWDSImporter\n",
        "\n",
        "input_dirs = {\"dir\": Path(\"unit_testing/assets/bop_wds_dataset\")}\n",
        "importer = BopWDSImporter(\n",
        "    name=\"Bop_WDS_test\",\n",
        "    description=\"List - bop_WDS\",\n",
        "    split=[\"test\", \"val\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with tempfile.TemporaryDirectory() as dir:\n",
        "    ds = importer.import_dataset(input_dirs, Path(dir) / \"wds\")\n",
        "    for r in ds.to_batches(limit=4):\n",
        "        R = r\n",
        "        break\n",
        "\n",
        "R.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for r in importer.import_row(input_dirs):\n",
        "    #print(r.to_pydict())\n",
        "    r1 = r\n",
        "    break\n",
        "\n",
        "r1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "field_dict = {\n",
        "    \"id\": \"str\",\n",
        "    \"rgb\": \"Image\",\n",
        "    \"depth\": \"DepthImage\",\n",
        "    \"camera\": \"Camera\",\n",
        "    \"category_id\": \"[int]\",\n",
        "    \"objects_id\": \"[str]\",\n",
        "    \"masks\": \"[CompressedRLE]\",\n",
        "    \"gt\": \"[Pose]\",\n",
        "    \"gt_info\": \"[GtInfo]\",\n",
        "    \"split\": \"str\",\n",
        "}\n",
        "\n",
        "bop_info = DatasetInfo(\n",
        "    id=\"0\", name=\"Bop\", description=\"Bop dataset\", fields=Fields.from_dict(field_dict)\n",
        ")\n",
        "\n",
        "bop_importer = BOPImporter(shard_split, bop_info, dir + \"/bop_ds.lance\")\n",
        "\n",
        "vars(bop_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#bop_ds = bop_importer.import_dataset()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
