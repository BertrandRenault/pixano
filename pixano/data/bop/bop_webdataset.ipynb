{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/maximilien/work/pixano/\")\n",
    "sys.path.append(\"/home/maximilien/work/lib/bop_toolkit\")\n",
    "\n",
    "dir = \"/home/maximilien/work/bop_data/\"\n",
    "coco = \"/home/maximilien/work/adapt-2023_with_gt/test/coco_gt.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lance\n",
    "import duckdb\n",
    "\n",
    "import bop_toolkit_lib.dataset.bop_webdataset as btk\n",
    "\n",
    "import webdataset as wds\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "from PIL import Image as pilImage\n",
    "import pyarrow as pa\n",
    "\n",
    "from pixano.core import Features\n",
    "from pixano.data import DataLoader\n",
    "from pixano import types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lance import LanceDataset\n",
    "from pixano.core.dataset import DatasetInfo\n",
    "from pixano.transforms.image import image_to_thumbnail, image_to_binary\n",
    "\n",
    "\n",
    "class BOPImporter(DataLoader):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        shard_split:dict[str, list[str]],\n",
    "        info:DatasetInfo,\n",
    "        target_dir: Path,\n",
    "    ):\n",
    "        \n",
    "        self.shard_split = shard_split\n",
    "        self.info = info\n",
    "        self.target_dir = target_dir\n",
    "\n",
    "    @property\n",
    "    def features(self) -> Features:\n",
    "        return Features.from_string_dict(self.info.features)\n",
    "\n",
    "    def create_json(self):\n",
    "        \"\"\"Create dataset spec.json\"\"\"\n",
    "\n",
    "        # Read dataset\n",
    "        #\"dataset = ds.dataset(self.target_dir + \"/db\", partitioning=self.partitioning)\n",
    "\n",
    "        # Check number of rows in the created dataset\n",
    "        #self.info.num_elements = dataset.count_rows()\n",
    "\n",
    "        # Create spec.json\n",
    "        with open(self.target_dir + \"/spec.json\", \"w\") as f:\n",
    "            json.dump(vars(self.info), f, indent=4)\n",
    "\n",
    "\n",
    "    def fields(self):\n",
    "        return self.features.to_fields()\n",
    "    \n",
    "    def schema(self):\n",
    "        return pa.schema(self.fields())\n",
    "    \n",
    "    def get_row(self):\n",
    "        \n",
    "        #split dataset\n",
    "        for split, shard_list in self.shard_split.items():\n",
    "            print(split)\n",
    "\n",
    "            _wds_pipeline = wds.DataPipeline(\n",
    "                wds.SimpleShardList(shard_list),\n",
    "                wds.tarfile_to_samples()\n",
    "            )\n",
    "            \n",
    "            #extract row of each split\n",
    "            for n, row in enumerate(_wds_pipeline):\n",
    "\n",
    "                if True:\n",
    "                    sample = btk.decode_sample(\n",
    "                        row,\n",
    "                        decode_camera=True,\n",
    "                        decode_rgb=True,\n",
    "                        decode_gray=False,\n",
    "                        decode_depth=True,\n",
    "                        decode_gt=True,\n",
    "                        decode_gt_info=True,\n",
    "                        decode_mask_visib=False,\n",
    "                        decode_mask=False,\n",
    "                        rgb_suffix='.png'\n",
    "                    )\n",
    "\n",
    "                    #id\n",
    "                    id = row[\"__key__\"]\n",
    "\n",
    "                    scene, image = id.split('_')\n",
    "                    coco_json_path = f'/home/maximilien/work/adapt-2023_with_gt/{split}/{scene}/scene_gt_coco.json'\n",
    "\n",
    "                    #rgb\n",
    "                    im_pil = pilImage.fromarray(sample['im_rgb'])\n",
    "\n",
    "                    im_pil = image_to_binary(im_pil, format='JPEG')\n",
    "                    \n",
    "                    preview = image_to_thumbnail(im_pil)\n",
    "\n",
    "                    rgb = types.Image(f\"\", im_pil, preview)\n",
    "                    rgbs = types.ImageType.Array.from_list([rgb])\n",
    "                    #dept\n",
    "                    depths = types.DepthImageType.Array.from_list([types.DepthImage(depth_map=sample[\"im_depth\"], shape=sample[\"im_depth\"].shape)])\n",
    "                    #camera\n",
    "                    cameras = types.CameraType.Array.from_list([types.Camera.from_dict(sample['camera'])])\n",
    "\n",
    "\n",
    "                #Objects\n",
    "                    nb_object = len(sample['gt'])\n",
    "                    #category\n",
    "                    category_id = [sample['gt'][i]['object_id'] for i in range(nb_object)]\n",
    "                    category_id_arr = pa.array([category_id])\n",
    "\n",
    "                    #pose\n",
    "                    gt = [types.Pose(sample['gt'][i]['cam_R_m2c'].flatten(), sample['gt'][i]['cam_t_m2c'].flatten()) for i in range(nb_object)]\n",
    "                    gt_arr = types.PoseType.Array.from_lists([gt])\n",
    "\n",
    "                    #gt_info\n",
    "                    gt_infos = [\n",
    "                        types.GtInfo.from_dict(\n",
    "                            {\n",
    "                                **sample[\"gt_info\"][i],\n",
    "                                \"bbox_obj\": types.BBox.from_xywh(sample[\"gt_info\"][i][\"bbox_obj\"]),\n",
    "                                \"bbox_visib\": types.BBox.from_xywh(sample[\"gt_info\"][i][\"bbox_visib\"]),\n",
    "                            }\n",
    "                        )\n",
    "                        for i in range(nb_object)\n",
    "                    ]\n",
    "                    gt_infos_arr = types.GtInfoType.Array.from_lists([gt_infos])\n",
    "\n",
    "                    #objects_ids and masks\n",
    "                    with open(coco_json_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "        \n",
    "                    object_ids = []\n",
    "                    masks = []\n",
    "                    for ann in data['annotations']:\n",
    "                        #check if same image key, then annotations are in same order as other object's attribute in coco.json\n",
    "                        if '000' + ann['image_id'] == id.replace('_','-'):\n",
    "                            object_ids.append(ann['id'])\n",
    "                            masks.append(types.CompressedRLE.from_urle(ann['segmentation'], ann['segmentation']['size'][0], ann['segmentation']['size'][1]))\n",
    "\n",
    "                    masks_arr = types.CompressedRLEType.Array.from_lists([masks])\n",
    "                    object_ids_arr = pa.array([object_ids])\n",
    "\n",
    "                #Struct array\n",
    "                    struct_arr = pa.StructArray.from_arrays(\n",
    "                        [\n",
    "                            pa.array([id]),\n",
    "                            rgbs,\n",
    "                            depths,\n",
    "                            cameras,\n",
    "                            category_id_arr,\n",
    "                            object_ids_arr,\n",
    "                            masks_arr,\n",
    "                            gt_arr,\n",
    "                            gt_infos_arr,\n",
    "                            pa.array([split])\n",
    "                        ],\n",
    "                        fields=self.fields()\n",
    "                    )\n",
    "\n",
    "                    yield pa.RecordBatch.from_struct_array(struct_arr)\n",
    "\n",
    "\n",
    "    def import_dataset(self, max_rows_per_file: int = 1024*1024, max_rows_per_group:int = 1024) -> LanceDataset:\n",
    "        \"\"\"Import dataset to Pixano format\n",
    "\n",
    "        Args:\n",
    "            batch_size (int, optional): Number of rows per file. Defaults to 2048.\n",
    "        \"\"\"\n",
    "        reader = pa.RecordBatchReader.from_batches(self.schema(), self.get_row())\n",
    "        ds = lance.write_dataset(reader, self.target_dir)\n",
    "\n",
    "        self.info.num_elements = ds.count_rows()\n",
    "\n",
    "        # Create spec.json\n",
    "        self.create_json()\n",
    "        \n",
    "        return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "shard_test_dir = \"/home/maximilien/work/adapt-2023_with_gt/shard/shard_test/\"\n",
    "shard_test_list = [os.path.join(shard_test_dir, shard) for shard in os.listdir(shard_test_dir) if shard.endswith(\".tar\")]\n",
    "\n",
    "shard_validation_dir = \"/home/maximilien/work/adapt-2023_with_gt/shard/shard_validation/\"\n",
    "shard_validation_list = [os.path.join(shard_validation_dir, shard) for shard in os.listdir(shard_validation_dir) if shard.endswith(\".tar\")]\n",
    "\n",
    "shard_split = {'test':shard_test_list, 'val': shard_validation_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {\n",
    "    'id': 'str',\n",
    "    'rgb': 'Image',\n",
    "    'depth': 'DepthImage',\n",
    "    'camera': 'Camera',\n",
    "    'category_id': '[int]',\n",
    "    'objects_id': '[str]',\n",
    "    'masks': '[CompressedRLE]',\n",
    "    'gt': '[Pose]',\n",
    "    'gt_info':'[GtInfo]',\n",
    "    'split':'str'\n",
    "}\n",
    "\n",
    "bop_info = DatasetInfo(id=\"0\", name=\"Bop\", description=\"Bop dataset\", features=features_dict)\n",
    "\n",
    "bop_importer = BOPImporter(shard_split, bop_info, dir + '/bop_ds.lance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximilien/miniconda3/envs/pixano/lib/python3.11/site-packages/imageio/plugins/pillow.py:297: UserWarning: Loading 16-bit (uint16) PNG as int32 due to limitations in pillow's PNG decoder. This will be fixed in a future version of pillow which will make this warning dissapear.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\n"
     ]
    }
   ],
   "source": [
    "bop_ds = bop_importer.import_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
