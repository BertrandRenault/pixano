{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/maximilien/work/pixano/\")\n",
    "sys.path.append(\"/home/maximilien/work/lib/bop_toolkit\")\n",
    "\n",
    "dir = \"/home/maximilien/work/bop_data/\"\n",
    "coco = \"/home/maximilien/work/adapt-2023_with_gt/test/coco_gt.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import bop_toolkit_lib.dataset.bop_webdataset as btk\n",
    "import lance\n",
    "import pyarrow as pa\n",
    "import webdataset as wds\n",
    "from lance import LanceDataset\n",
    "from PIL import Image as pilImage\n",
    "\n",
    "from pixano.data import DatasetInfo\n",
    "from pixano.data.importers import DataImporter\n",
    "from pixano.types import (\n",
    "    BBox,\n",
    "    Camera,\n",
    "    CameraType,\n",
    "    CompressedRLE,\n",
    "    CompressedRLEType,\n",
    "    DepthImage,\n",
    "    DepthImageType,\n",
    "    Fields,\n",
    "    GtInfo,\n",
    "    GtInfoType,\n",
    "    Image,\n",
    "    ImageType,\n",
    "    Pose,\n",
    "    PoseType,\n",
    ")\n",
    "from pixano.utils import image_to_binary, image_to_thumbnail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOPImporter(DataImporter):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        shard_split:dict[str, list[str]],\n",
    "        info:DatasetInfo,\n",
    "        target_dir: Path,\n",
    "    ):\n",
    "        \n",
    "        self.shard_split = shard_split\n",
    "        self.info = info\n",
    "        self.target_dir = target_dir\n",
    "\n",
    "    @property\n",
    "    def fields(self) -> Fields:\n",
    "        return Fields.from_string_dict(self.info.fields)\n",
    "\n",
    "    def create_json(self):\n",
    "        \"\"\"Create dataset spec.json\"\"\"\n",
    "\n",
    "        # Read dataset\n",
    "        #\"dataset = ds.dataset(self.target_dir + \"/db\", partitioning=self.partitioning)\n",
    "\n",
    "        # Check number of rows in the created dataset\n",
    "        #self.info.num_elements = dataset.count_rows()\n",
    "\n",
    "        # Create spec.json\n",
    "        with open(self.target_dir + \"/spec.json\", \"w\") as f:\n",
    "            json.dump(vars(self.info), f, indent=4)\n",
    "\n",
    "\n",
    "    def fields(self):\n",
    "        return self.features.to_fields()\n",
    "    \n",
    "    def schema(self):\n",
    "        return pa.schema(self.fields())\n",
    "    \n",
    "    def get_row(self):\n",
    "        \n",
    "        #split dataset\n",
    "        for split, shard_list in self.shard_split.items():\n",
    "            print(split)\n",
    "\n",
    "            _wds_pipeline = wds.DataPipeline(\n",
    "                wds.SimpleShardList(shard_list),\n",
    "                wds.tarfile_to_samples()\n",
    "            )\n",
    "            \n",
    "            #extract row of each split\n",
    "            for n, row in enumerate(_wds_pipeline):\n",
    "\n",
    "                if True:\n",
    "                    sample = btk.decode_sample(\n",
    "                        row,\n",
    "                        decode_camera=True,\n",
    "                        decode_rgb=True,\n",
    "                        decode_gray=False,\n",
    "                        decode_depth=True,\n",
    "                        decode_gt=True,\n",
    "                        decode_gt_info=True,\n",
    "                        decode_mask_visib=False,\n",
    "                        decode_mask=False,\n",
    "                        rgb_suffix='.png'\n",
    "                    )\n",
    "\n",
    "                    #id\n",
    "                    id = row[\"__key__\"]\n",
    "\n",
    "                    scene, image = id.split('_')\n",
    "                    coco_json_path = f'/home/maximilien/work/adapt-2023_with_gt/{split}/{scene}/scene_gt_coco.json'\n",
    "\n",
    "                    #rgb\n",
    "                    im_pil = pilImage.fromarray(sample['im_rgb'])\n",
    "\n",
    "                    im_pil = image_to_binary(im_pil, format='JPEG')\n",
    "                    \n",
    "                    preview = image_to_thumbnail(im_pil)\n",
    "\n",
    "                    rgb = Image(f\"\", im_pil, preview)\n",
    "                    rgbs = ImageType.Array.from_list([rgb])\n",
    "                    #dept\n",
    "                    depths = DepthImageType.Array.from_list([DepthImage(depth_map=sample[\"im_depth\"], shape=sample[\"im_depth\"].shape)])\n",
    "                    #camera\n",
    "                    cameras = CameraType.Array.from_list([Camera.from_dict(sample['camera'])])\n",
    "\n",
    "\n",
    "                #Objects\n",
    "                    nb_object = len(sample['gt'])\n",
    "                    #category\n",
    "                    category_id = [sample['gt'][i]['object_id'] for i in range(nb_object)]\n",
    "                    category_id_arr = pa.array([category_id])\n",
    "\n",
    "                    #pose\n",
    "                    gt = [Pose(sample['gt'][i]['cam_R_m2c'].flatten(), sample['gt'][i]['cam_t_m2c'].flatten()) for i in range(nb_object)]\n",
    "                    gt_arr = PoseType.Array.from_lists([gt])\n",
    "\n",
    "                    #gt_info\n",
    "                    gt_infos = [\n",
    "                        GtInfo.from_dict(\n",
    "                            {\n",
    "                                **sample[\"gt_info\"][i],\n",
    "                                \"bbox_obj\": BBox.from_xywh(sample[\"gt_info\"][i][\"bbox_obj\"]),\n",
    "                                \"bbox_visib\": BBox.from_xywh(sample[\"gt_info\"][i][\"bbox_visib\"]),\n",
    "                            }\n",
    "                        )\n",
    "                        for i in range(nb_object)\n",
    "                    ]\n",
    "                    gt_infos_arr = GtInfoType.Array.from_lists([gt_infos])\n",
    "\n",
    "                    #objects_ids and masks\n",
    "                    with open(coco_json_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "        \n",
    "                    object_ids = []\n",
    "                    masks = []\n",
    "                    for ann in data['annotations']:\n",
    "                        #check if same image key, then annotations are in same order as other object's attribute in coco.json\n",
    "                        if '000' + ann['image_id'] == id.replace('_','-'):\n",
    "                            object_ids.append(ann['id'])\n",
    "                            masks.append(CompressedRLE.from_urle(ann['segmentation'], ann['segmentation']['size'][0], ann['segmentation']['size'][1]))\n",
    "\n",
    "                    masks_arr = CompressedRLEType.Array.from_lists([masks])\n",
    "                    object_ids_arr = pa.array([object_ids])\n",
    "\n",
    "                #Struct array\n",
    "                    struct_arr = pa.StructArray.from_arrays(\n",
    "                        [\n",
    "                            pa.array([id]),\n",
    "                            rgbs,\n",
    "                            depths,\n",
    "                            cameras,\n",
    "                            category_id_arr,\n",
    "                            object_ids_arr,\n",
    "                            masks_arr,\n",
    "                            gt_arr,\n",
    "                            gt_infos_arr,\n",
    "                            pa.array([split])\n",
    "                        ],\n",
    "                        fields=self.fields()\n",
    "                    )\n",
    "\n",
    "                    yield pa.RecordBatch.from_struct_array(struct_arr)\n",
    "\n",
    "\n",
    "    def import_dataset(self, max_rows_per_file: int = 1024*1024, max_rows_per_group:int = 1024) -> LanceDataset:\n",
    "        \"\"\"Import dataset to Pixano format\n",
    "\n",
    "        Args:\n",
    "            batch_size (int, optional): Number of rows per file. Defaults to 2048.\n",
    "        \"\"\"\n",
    "\n",
    "        reader = pa.RecordBatchReader.from_batches(self.schema(), self.get_row())\n",
    "        ds = lance.write_dataset(reader, self.target_dir)\n",
    "\n",
    "        self.info.num_elements = ds.count_rows()\n",
    "\n",
    "        # Create spec.json\n",
    "        self.create_json()\n",
    "        \n",
    "        return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "shard_test_dir = \"/home/maximilien/work/adapt-2023_with_gt/shard/shard_test/\"\n",
    "shard_test_list = [os.path.join(shard_test_dir, shard) for shard in os.listdir(shard_test_dir) if shard.endswith(\".tar\")]\n",
    "\n",
    "shard_validation_dir = \"/home/maximilien/work/adapt-2023_with_gt/shard/shard_validation/\"\n",
    "shard_validation_list = [os.path.join(shard_validation_dir, shard) for shard in os.listdir(shard_validation_dir) if shard.endswith(\".tar\")]\n",
    "\n",
    "shard_split = {'test':shard_test_list, 'val': shard_validation_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_dict = {\n",
    "    'id': 'str',\n",
    "    'rgb': 'Image',\n",
    "    'depth': 'DepthImage',\n",
    "    'camera': 'Camera',\n",
    "    'category_id': '[int]',\n",
    "    'objects_id': '[str]',\n",
    "    'masks': '[CompressedRLE]',\n",
    "    'gt': '[Pose]',\n",
    "    'gt_info':'[GtInfo]',\n",
    "    'split':'str'\n",
    "}\n",
    "\n",
    "bop_info = DatasetInfo(id=\"0\", name=\"Bop\", description=\"Bop dataset\", fields=field_dict)\n",
    "\n",
    "bop_importer = BOPImporter(shard_split, bop_info, dir + '/bop_ds.lance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximilien/miniconda3/envs/pixano/lib/python3.11/site-packages/imageio/plugins/pillow.py:297: UserWarning: Loading 16-bit (uint16) PNG as int32 due to limitations in pillow's PNG decoder. This will be fixed in a future version of pillow which will make this warning dissapear.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\n"
     ]
    }
   ],
   "source": [
    "bop_ds = bop_importer.import_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
